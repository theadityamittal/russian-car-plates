{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00b2d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone # For cloning estimators in cross-validation\n",
    "from sklearn.compose import ColumnTransformer # To apply different transformers to different columns\n",
    "from sklearn.pipeline import Pipeline # To chain multiple processing steps and a final estimator\n",
    "from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer, OneHotEncoder # Various encoding/discretization methods\n",
    "from sklearn.model_selection import StratifiedKFold # Cross-validation strategy\n",
    "from xgboost import XGBRegressor # Gradient Boosting Machine from XGBoost\n",
    "from lightgbm import LGBMRegressor # Gradient Boosting Machine from LightGBM\n",
    "from catboost import CatBoostRegressor # Gradient Boosting Machine from CatBoost\n",
    "import category_encoders as ce # Advanced categorical encoders (install with: pip install category-encoders)\n",
    "\n",
    "# General Parameters for reproducibility and control\n",
    "SEED = 92       # Random seed for reproducibility\n",
    "N_SPLITS = 10   # Number of folds for cross-validation\n",
    "\n",
    "TARGET = 'log_price' # The logarithmically transformed target variable for modeling\n",
    "\n",
    "# Columns to be dropped from the feature set (X) before training\n",
    "# These include original identifiers, the original price, and engineered features\n",
    "# that might be redundant or explicitly excluded from the model.\n",
    "DROP_COLS = [\n",
    "    'id', 'plate', 'price', 'log_price', 'is_train', # Essential IDs and target variables\n",
    "    # Specific engineered features that might be dropped if they are redundant or not performing well:\n",
    "    \"is_number_000\", \"is_number_444\", \"is_number_222\", \"is_number_700\", \n",
    "    \"is_number_555\", \"quarter\", \"day_of_week\", \"is_weekend\", # Time-based features\n",
    "    \"prestige_score\", # Dropping the combined score if its components are used directly or if it leads to multicollinearity\n",
    "    \"is_number_300\",\"is_number_333\",\"is_number_400\" # Potentially redundant number pattern flags\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec1a7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('data/processed_train.pkl')\n",
    "test_df = pd.read_pickle('data/processed_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae49530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training and testing sets.\n",
      "Training features (X) shape: (51640, 50)\n",
      "Training target (y) shape: (51640,)\n",
      "Test features (X_test) shape: (7695, 50)\n"
     ]
    }
   ],
   "source": [
    "# Defining the features (X) and the target (y) for the training set,\n",
    "# and features for the test set (X_test).\n",
    "# 'errors='ignore'' handles cases where a column in DROP_COLS might not exist, preventing errors.\n",
    "X = train_df.drop(columns=DROP_COLS, errors='ignore')\n",
    "y = train_df[TARGET].copy()\n",
    "X_test = test_df.drop(columns=DROP_COLS, errors='ignore')\n",
    "\n",
    "print(\"Data split into training and testing sets.\")\n",
    "print(f\"Training features (X) shape: {X.shape}\")\n",
    "print(f\"Training target (y) shape: {y.shape}\")\n",
    "print(f\"Test features (X_test) shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52cd62e",
   "metadata": {},
   "source": [
    "#### Automatic Column Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8669e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Summary ➜ Numerical: 24 | Boolean: 16 | Low Cardinality Categorical: 3 | Medium Cardinality Categorical: 3 | High Cardinality Categorical: 4\n"
     ]
    }
   ],
   "source": [
    "# This section is crucial for handling different data types dynamically.\n",
    "# It automatically identifies numerical, boolean, and categorical columns,\n",
    "# and further segments categorical columns by their cardinality to apply\n",
    "# appropriate encoding strategies.\n",
    "\n",
    "def detect_columns(X):\n",
    "    \"\"\"\n",
    "    Detects and segments columns by their data type and cardinality.\n",
    "    This helps in applying specific preprocessing steps to different column types.\n",
    "    \"\"\"\n",
    "    bool_cols = [c for c in X.columns if X[c].dtype == 'bool'] # Identify boolean columns\n",
    "    num_cols = [c for c in X.columns if X[c].dtype.kind in 'if' and c not in bool_cols] # Identify numerical (int/float) columns, excluding booleans\n",
    "    cat_cols = [c for c in X.columns if c not in num_cols + bool_cols]  # Remaining columns are treated as categorical\n",
    "\n",
    "    # Further segmentation of categorical columns by cardinality (number of unique values)\n",
    "    # Different encoding strategies are optimal for different cardinalities.\n",
    "    cat_low = [c for c in cat_cols if X[c].nunique() <= 20] # Low cardinality for One-Hot Encoding\n",
    "    cat_mid = [c for c in cat_cols if 20 < X[c].nunique() <= 200] # Medium cardinality for Ordinal Encoding\n",
    "    cat_high = [c for c in cat_cols if X[c].nunique() > 200] # High cardinality for Target Encoding\n",
    "\n",
    "    print('\\nColumn Summary ➜ Numerical:', len(num_cols),\n",
    "          '| Boolean:', len(bool_cols),\n",
    "          '| Low Cardinality Categorical:', len(cat_low),\n",
    "          '| Medium Cardinality Categorical:', len(cat_mid),\n",
    "          '| High Cardinality Categorical:', len(cat_high))\n",
    "\n",
    "    return num_cols, bool_cols, cat_low, cat_mid, cat_high\n",
    "\n",
    "# Apply the column detection function to the training features\n",
    "num_cols, bool_cols, cat_low, cat_mid, cat_high = detect_columns(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc186b0",
   "metadata": {},
   "source": [
    "#### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bac2a827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing pipeline (ColumnTransformer) defined.\n"
     ]
    }
   ],
   "source": [
    "# The `ColumnTransformer` is the core component here. It allows applying\n",
    "# different transformations to different subsets of columns in parallel.\n",
    "# This ensures that each column type is handled appropriately before feeding to the model.\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', num_cols), # Numerical columns: 'passthrough' means no transformation\n",
    "        ('bool', 'passthrough', bool_cols), # Boolean columns: 'passthrough' as they are already binary\n",
    "        ('low', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_low),  # One-Hot Encoding for low cardinality categories\n",
    "                                                                                        # 'handle_unknown='ignore'' prevents errors if new categories appear in test set\n",
    "                                                                                        # 'sparse_output=False' returns a dense NumPy array\n",
    "        ('mid', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_mid),  # Ordinal Encoding for medium cardinality categories\n",
    "                                                                                                # Assigns a unique integer to each category.\n",
    "                                                                                                # 'unknown_value=-1' handles unseen categories.\n",
    "        ('high', ce.TargetEncoder(cols=cat_high, smoothing=0.2), cat_high)  # Target Encoding for high cardinality categories\n",
    "                                                                            # Replaces category with mean of target. 'smoothing' helps prevent overfitting.\n",
    "    ],\n",
    "    remainder='drop',  # Drops any columns not explicitly specified in `transformers` (safer approach)\n",
    "    n_jobs=-1  # Utilizes all available CPU cores for parallel processing during transformation\n",
    ")\n",
    "print(\"\\nPreprocessing pipeline (ColumnTransformer) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc16fed",
   "metadata": {},
   "source": [
    "#### MODELS AND PARAMETERS (Optimized and Modular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b20705f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models and their optimized hyperparameters defined.\n",
      "Pipelines constructed: Preprocessing -> Model.\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Parameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 1433,\n",
    "    'max_depth': 12,\n",
    "    'learning_rate': 0.01852160907217988,\n",
    "    'subsample': 0.6786672470738663,\n",
    "    'colsample_bytree': 0.46208650739218005,\n",
    "    'reg_alpha': 0.017519138973638618,\n",
    "    'reg_lambda': 0.2839310763317462,\n",
    "    'gamma': 0.0033995958574628547,\n",
    "    'tweedie_variance_power': 1.0869464555654937, # Tweedie objective is suitable for target variables with a skewed distribution and many zero values, which can be the case for prices.\n",
    "    'objective': 'reg:tweedie',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# LightGBM Parameters\n",
    "lgb_params = {\n",
    "    'n_estimators': 999,\n",
    "    'max_depth': 11,\n",
    "    'learning_rate': 0.07607568555547708,\n",
    "    'subsample': 0.6363036032688429,\n",
    "    'colsample_bytree': 0.5072021102992719,\n",
    "    'min_child_samples': 97,\n",
    "    'reg_alpha': 0.16671454380081874,\n",
    "    'reg_lambda': 0.6455320711051608,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# CatBoost Parameters\n",
    "cat_params = {\n",
    "    'iterations': 991,\n",
    "    'depth': 10,\n",
    "    'learning_rate': 0.06462213707942074,\n",
    "    'l2_leaf_reg': 1.9289204888270515,\n",
    "    'subsample': 0.7213225292844163,\n",
    "    'bagging_temperature': 0.4361642090192932,\n",
    "    'random_strength': 6.443179917768372,\n",
    "    'min_data_in_leaf': 72,\n",
    "    'loss_function': 'RMSE', # Root Mean Squared Error, common for regression tasks\n",
    "    'verbose': 0, # Suppress training output for cleaner logs\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# Dictionary of models to be trained. Easily extensible to include more models.\n",
    "# Uncomment LGBM and CatBoost to include them in the ensemble.\n",
    "models = {\n",
    "    'XGB': XGBRegressor(**xgb_params),\n",
    "    #'LGBM': LGBMRegressor(**lgb_params),\n",
    "    #'CatBoost': CatBoostRegressor(**cat_params)\n",
    "}\n",
    "print(\"\\nModels and their optimized hyperparameters defined.\")\n",
    "\n",
    "# Construct the full pipeline for each model: preprocessing + estimator\n",
    "# Each pipeline handles all necessary data transformations before training the model.\n",
    "pipelines = {name: Pipeline(steps=[('prep', preprocess), ('model', model)]) for name, model in models.items()}\n",
    "print(\"Pipelines constructed: Preprocessing -> Model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2541f6",
   "metadata": {},
   "source": [
    "#### SMAPE METRIC Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d0670e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMAPE evaluation metric defined.\n"
     ]
    }
   ],
   "source": [
    "# The Symmetric Mean Absolute Percentage Error (SMAPE) is often used in forecasting\n",
    "# and is robust to zero values in the actuals. It's defined once to ensure consistency.\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
    "    Formula: (1/n) * Sum(|y_true - y_pred| / ((|y_true| + |y_pred|) / 2)) * 100\n",
    "    This metric handles cases where y_true or y_pred (or both) are zero.\n",
    "    \"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Handle division by zero: if denominator is zero (i.e., both y_true and y_pred are zero),\n",
    "    # the corresponding term for SMAPE is defined as 0.0 to avoid NaN/Inf.\n",
    "    smape_term = np.zeros_like(diff, dtype=float)\n",
    "    non_zero_denom = denominator != 0 # Identify where denominator is not zero\n",
    "    smape_term[non_zero_denom] = diff[non_zero_denom] / denominator[non_zero_denom]\n",
    "    \n",
    "    return np.mean(smape_term) * 100\n",
    "\n",
    "print(\"\\nSMAPE evaluation metric defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e924851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target variable ('log_price') binned into 10 strata for stratification.\n",
      "\n",
      "===== CROSS-VALIDATION TRAINING =====\n",
      "\n",
      "Initiating training for model: XGB...\n",
      "  Fold 01/10\n",
      "  Fold 02/10\n",
      "  Fold 03/10\n",
      "  Fold 04/10\n",
      "  Fold 05/10\n",
      "  Fold 06/10\n",
      "  Fold 07/10\n",
      "  Fold 08/10\n",
      "  Fold 09/10\n",
      "  Fold 10/10\n",
      "⮕  Overall CV SMAPE for XGB: 35.60%\n"
     ]
    }
   ],
   "source": [
    "# Stratified K-Fold cross-validation is used to ensure that each fold has a\n",
    "# representative distribution of the target variable. This is especially important\n",
    "# for skewed targets or when specific target ranges are more critical.\n",
    "\n",
    "# Bin the target variable ('log_price') to create \"strata\" for StratifiedKFold.\n",
    "# This effectively treats the regression problem as a classification problem for splitting purposes,\n",
    "# ensuring similar target distributions across folds.\n",
    "y_bins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile') \\\n",
    "    .fit_transform(y.values.reshape(-1, 1)).astype(int).ravel()\n",
    "print(f\"\\nTarget variable ('{TARGET}') binned into {y_bins.max() + 1} strata for stratification.\")\n",
    "\n",
    "# Initialize StratifiedKFold with the specified number of splits and random state.\n",
    "kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Dictionaries to store out-of-fold (OOF) predictions and test predictions for each model.\n",
    "# OOF predictions are used for ensemble weighting and final CV evaluation.\n",
    "# Test predictions are accumulated across folds for final submission.\n",
    "oof_preds = {name: np.zeros(len(y)) for name in models}\n",
    "test_preds = {name: np.zeros(len(X_test)) for name in models}\n",
    "feature_importances = {} # To store feature importances for each model (if available)\n",
    "\n",
    "print('\\n===== CROSS-VALIDATION TRAINING =====')\n",
    "# Iterate through each defined model and perform cross-validation\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"\\nInitiating training for model: {model_name}...\")\n",
    "    try:\n",
    "        # Loop through each fold generated by StratifiedKFold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y_bins), 1):\n",
    "            print(f\"  Fold {fold:02d}/{N_SPLITS}\")\n",
    "            X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx] # Training data for the current fold\n",
    "            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx] # Validation data for the current fold\n",
    "\n",
    "            # Train the pipeline on the training data\n",
    "            pipeline.fit(X_tr, y_tr)\n",
    "            \n",
    "            # Store out-of-fold (OOF) predictions for the validation set\n",
    "            oof_preds[model_name][val_idx] = pipeline.predict(X_val)\n",
    "            \n",
    "            # Accumulate test predictions by averaging predictions from each fold\n",
    "            # This is a simple form of ensemble averaging.\n",
    "            test_preds[model_name] += pipeline.predict(X_test) / N_SPLITS\n",
    "\n",
    "        # Calculate the overall cross-validation SMAPE for the current model\n",
    "        # Predictions are converted back from log-price scale to original price scale for SMAPE calculation.\n",
    "        cv_smape = smape(np.exp(y), np.exp(oof_preds[model_name]))\n",
    "        print(f'⮕  Overall CV SMAPE for {model_name}: {cv_smape:.2f}%')\n",
    "\n",
    "        # Extract feature importances from the trained model (if the model supports it)\n",
    "        if hasattr(pipeline['model'], 'feature_importances_'):\n",
    "            # For models like LightGBM, scikit-learn tree models\n",
    "            feature_importances[model_name] = pipeline['model'].feature_importances_\n",
    "        elif hasattr(pipeline['model'], 'get_booster'): \n",
    "            # For XGBoost, get_booster() allows accessing internal booster attributes\n",
    "            feature_importances[model_name] = pipeline['model'].get_booster().get_score(importance_type='weight') \n",
    "            # 'importance_type' can be 'weight' (number of times a feature is used in splits), 'gain' (average gain across splits), 'cover', etc.\n",
    "        else:\n",
    "            feature_importances[model_name] = None # No direct importance available\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training of model {model_name}: {str(e)}\")\n",
    "        continue  # Skip to the next model in case of an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df77430",
   "metadata": {},
   "source": [
    "#### ENSEMBLE PREDICTIONS (Inverse Error Weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "491b359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⮕  Final Ensemble Model CV SMAPE: 35.60%\n",
      "\n",
      "Model Weights in Ensemble:\n",
      "- XGB: 1.0000 (Individual OOF SMAPE: 35.60%)\n"
     ]
    }
   ],
   "source": [
    "# This ensembling method is simple yet effective: models that perform better\n",
    "# (i.e., have a lower SMAPE on the validation set) are given higher weights\n",
    "# in the final blended prediction.\n",
    "\n",
    "# Calculate errors (SMAPE) for each model based on their OOF predictions\n",
    "errors = {name: smape(np.exp(y), np.exp(oof_preds[name])) for name in models}\n",
    "\n",
    "# Calculate inverse errors (1 / SMAPE). A lower SMAPE means a higher inverse error.\n",
    "inv_errors = {k: 1 / v for k, v in errors.items()}\n",
    "\n",
    "# Normalize inverse errors to get weights that sum to 1.\n",
    "# These normalized weights determine each model's contribution to the final ensemble.\n",
    "norm_weights = {k: v / sum(inv_errors.values()) for k, v in inv_errors.items()}\n",
    "\n",
    "# Combine test predictions from individual models using the calculated normalized weights.\n",
    "# The predictions are combined in the log-price space, then converted back to original price.\n",
    "ensemble_preds_log = sum(test_preds[k] * norm_weights[k] for k in test_preds)\n",
    "ensemble_preds = np.exp(ensemble_preds_log) # Convert back from log-price to original price scale\n",
    "\n",
    "# Calculate the SMAPE of the ensemble model on the out-of-fold validation set.\n",
    "# This gives an estimate of the ensemble's performance on unseen data.\n",
    "ensemble_oof_log = sum(oof_preds[k] * norm_weights[k] for k in oof_preds)\n",
    "ensemble_smape = smape(np.exp(y), np.exp(ensemble_oof_log))\n",
    "print(f\"\\n⮕  Final Ensemble Model CV SMAPE: {ensemble_smape:.2f}%\")\n",
    "\n",
    "# Display the weights of each model in the ensemble, along with their individual SMAPE.\n",
    "print(\"\\nModel Weights in Ensemble:\")\n",
    "for model_name, weight in norm_weights.items():\n",
    "    print(f\"- {model_name}: {weight:.4f} (Individual OOF SMAPE: {errors[model_name]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7deb9c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅  Submission file \"submission.csv\" saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Final steps to prepare the predictions for submission to Kaggle.\n",
    "\n",
    "# Apply clipping to the ensemble predictions to prevent unrealistic values.\n",
    "# 'a_min=0' ensures no negative prices, while 'a_max=None' means no upper limit is applied here.\n",
    "# Clipping helps to make predictions more robust to potential outliers or model errors.\n",
    "ensemble_preds = np.clip(ensemble_preds, a_min=0, a_max=None)\n",
    "\n",
    "# Create the submission DataFrame with 'id' and the final 'price' predictions.\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'], # Ensure 'id' column is taken from the original test_df\n",
    "    'price': ensemble_preds\n",
    "})\n",
    "\n",
    "# Save the submission file to a CSV in the required format.\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('\\n✅  Submission file \"submission.csv\" saved successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
